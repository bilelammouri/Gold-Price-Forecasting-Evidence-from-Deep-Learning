# -*- coding: utf-8 -*-
"""modeling_cnn_without_docs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o_FeBywCnaMmqhElTRhvKsX9EzkEMW92
"""

import os
import json
import numpy as np
import pandas as pd
import kerastuner as kt
from typing import Dict, List
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv1D, Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# CNN fiting model and optimizer

def tune_hyperparameters(data, n_steps, dataset_name, sheet_name, max_epochs, factor):
    # Create sequences
    def create_sequences(data, n_steps):
        X, y = [], []
        for i in range(len(data) - n_steps):
            end_ix = i + n_steps
            if end_ix + 1 < len(data):
                X.append(data[i:end_ix])
                y.append(data[end_ix])
        return np.array(X), np.array(y)

    # Build model function
    def build_model(hp):
        model = Sequential()
        model.add(Conv1D(
            filters=hp.Int('filters', min_value=32, max_value=128, step=32),
            kernel_size=hp.Int('kernel_size', min_value=2, max_value=5),
            activation='relu',
            input_shape=(n_steps, 1)
        ))
        model.add(Flatten())
        model.add(Dense(hp.Int('dense_units', min_value=50, max_value=100, step=50), activation='relu'))
        model.add(Dense(1))

        model.compile(
            optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),
            loss='mse',
            metrics=['mae']
        )
        return model

    # Prepare data
    X, y = create_sequences(data, n_steps)
    X = X.reshape((X.shape[0], X.shape[1], 1))
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

    # Hyperparameter tuning
    tuner = kt.Hyperband(
        build_model,
        objective='val_loss',
        max_epochs=max_epochs,
        factor=factor,
        directory=f'kt_dir/{dataset_name}/{sheet_name}',
        project_name='gold_price_tuning'
    )

    tuner.search(X_train, y_train, epochs=200, validation_split=0.2)
    best_hyperparameters = tuner.get_best_hyperparameters()[0]
    best_model = tuner.get_best_models(num_models=1)[0]

    # Save the best model in Keras format
    best_model.save(f'best_model_{dataset_name}_{sheet_name}.keras')

    # Save the architecture of the model as a JPEG
    plot_model(best_model, to_file=f'model_architecture_{dataset_name}_{sheet_name}.png', show_shapes=True, show_layer_names=True)

    # Save the best hyperparameters as a JSON file
    with open(f'best_hyperparameters_{dataset_name}_{sheet_name}.json', 'w') as f:
        json.dump(best_hyperparameters.values, f)

    return best_hyperparameters, best_model

#
def load_keras_models(dataset_names, sheet_names, model_dir='.'):
    """
    Load Keras models from .keras files based on dataset and sheet names.

    Args:
        dataset_names (list): List of dataset names.
        sheet_names (list): List of sheet names.
        model_dir (str): Directory where the model files are stored (default: current directory).

    Returns:
        dict: A dictionary of loaded Keras models with keys formatted as 'cnn_model_{dataset_name}_{sheet_name}'.
    """
    models = {}

    # Loop over dataset and sheet names to load corresponding Keras models
    for dataset in dataset_names:
        for sheet in sheet_names:
            # Construct the file name and full path
            file_name = f'best_model_{dataset}_{sheet}.keras'
            file_path = os.path.join(model_dir, file_name)

            # Check if the model file exists
            if os.path.exists(file_path):
                # Load the model using Keras
                model = load_model(file_path)

                # Create a key name for the model in the dictionary
                model_name = f'cnn_model_{dataset}_{sheet}'

                # Store the model in the dictionary
                models[model_name] = model
            else:
                print(f"File {file_path} not found")

    return models

# Forecasting head step CNN

def create_sequences(data, n_steps):
    X, y = [], []
    for i in range(len(data) - n_steps):
        X.append(data[i:i + n_steps])
        y.append(data[i + n_steps])
    return np.array(X), np.array(y)

def build_and_train_model_a(X_train, y_train, n_steps, filters=64, kernel_size=2, dense_units=50, epochs=200, optimizer='adam', loss='mse'):
    model = Sequential([
        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=(n_steps, 1)),
        Flatten(),
        Dense(dense_units, activation='relu'),
        Dense(1)  # Predicting 1 time step ahead
    ])
    model.compile(optimizer=optimizer, loss=loss)
    model.fit(X_train, y_train, epochs=epochs, verbose=0)
    return model



def build_and_train_model(X_train, y_train, n_steps):
    model = Sequential([
        Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, 1)),
        Flatten(),
        Dense(50, activation='relu'),
        Dense(1)  # Predicting 1 time step ahead
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X_train, y_train, epochs=200, verbose=0)
    return model

def forecast_for_test_set_lastversion(model, test_data, n_steps, forecast_horizon):
    predictions = []
    for i in range(len(test_data) - n_steps):
        input_seq = test_data[i:i + n_steps].reshape((1, n_steps, 1))
        step_predictions = []
        for step in range(forecast_horizon):
            pred = model.predict(input_seq, verbose=0)
            step_predictions.append(pred[0, 0])
            input_seq = np.append(input_seq[:, 1:, :], pred.reshape(1, 1, 1), axis=1)
        predictions.append(step_predictions)
    return np.array(predictions)

def forecast_for_test_set(model, test_data, n_steps, forecast_horizon):
    predictions = []

    for i in range(len(test_data) - n_steps):
        input_seq = test_data[i:i + n_steps].reshape((1, n_steps, 1))  # Get the input sequence for this prediction window
        step_predictions = []  # Store predictions for the next `forecast_horizon` steps

        for step in range(forecast_horizon):
            # Predict the next time step
            pred = model.predict(input_seq, verbose=0)

            # Append the prediction to step_predictions
            step_predictions.append(pred[0, 0])

            # Update the input sequence by removing the oldest time step and adding the new prediction
            input_seq = np.append(input_seq[:, 1:, :], pred.reshape(1, 1, 1), axis=1)

        # After predicting `forecast_horizon` steps ahead, save the step_predictions
        predictions.append(step_predictions)

    return np.array(predictions)


def save_dataframe_to_excel(df, db_name, sheet_name):
    file_name = f"CNN_{db_name}_{sheet_name}_results.xlsx"
    df.to_excel(file_name, index=False)
    print(f"Saved results to {file_name}")

def process_database_sheets(data_dicts, sheets_to_read, forecast_horizon=24):
    results = {}

    # Define n_steps for each sheet
    n_steps_map = {
        'Yearly_EoP': 2,
        'Quarterly_EoP': 4,
        'Monthly_EoP': 12,
        'Weekly_EoP': 58,
        'Daily': 365
    }

    for db_name, db_data in data_dicts.items():
        db_results = {}
        for sheet in sheets_to_read:
            # Get the corresponding n_steps for the current sheet
            n_steps = n_steps_map.get(sheet, 12)  # Default to 12 if the sheet is not in the map

            # Step 1: Extract data for the current sheet
            data = db_data[sheet]

            # Ensure only numeric data is selected and ensure it's a 2D array
            if isinstance(data, pd.DataFrame):
                data_numeric = data.select_dtypes(include=[np.number]).values
            else:
                raise ValueError(f"Expected a DataFrame for sheet {sheet}, but got {type(data)}.")

            if data_numeric.ndim == 1:
                data_numeric = data_numeric.reshape(-1, 1)

            # Step 2: Preprocess (Scaling)
            scaler = MinMaxScaler()
            if len(data_numeric) > 0:
                data_scaled = scaler.fit_transform(data_numeric)
            else:
                continue  # Skip processing if the data is empty

            # Step 3: Split into train/test
            train_size = int(len(data_scaled) * 0.7)
            train, test = data_scaled[:train_size], data_scaled[train_size:]

            if len(train) == 0 or len(test) == 0:
                continue  # Skip this sheet if there's no data to train or test

            # Step 4: Create sequences
            X_train, y_train = create_sequences(train, n_steps)
            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # Reshape for Conv1D
            X_test, y_test = create_sequences(test, n_steps)

            if X_train.size == 0 or X_test.size == 0:
                continue  # Skip if no valid sequences could be created

            # Step 5: Build and train the model
            model = build_and_train_model(X_train, y_train, n_steps)

            # Step 6: Forecast for each test observation
            test_predictions = forecast_for_test_set(model, test, n_steps, forecast_horizon)

            # Step 7: Inverse transform the predictions and y_test
            test_predictions = scaler.inverse_transform(test_predictions)
            y_test = scaler.inverse_transform(y_test.reshape(-1, 1))

            # Step 8: Save the results in a DataFrame
            nb_obs = test_predictions.shape[0]
            columns = [f'head_{i+1}' for i in range(forecast_horizon)]
            forecast_df = pd.DataFrame(test_predictions, columns=columns)
            forecast_df.insert(0, 'nb.obs', np.arange(1, nb_obs + 1))
            forecast_df.insert(1, 'y_test', y_test[:nb_obs, 0])

            # Save results for this sheet
            save_dataframe_to_excel(forecast_df, db_name, sheet)

            # Save results for this sheet in results dictionary
            db_results[sheet] = forecast_df

        # Save results for this database
        results[db_name] = db_results

    return results


def process_database_sheets_a(data_dicts, sheets_to_read, df_cnn_hyperparameters, forecast_horizon=24):
    results = {}

    # Define n_steps for each sheet
    n_steps_map = {
        'Yearly_EoP': 2,
        'Quarterly_EoP': 4,
        'Monthly_EoP': 12,
        'Weekly_EoP': 58,
        'Daily': 365
    }

    for db_name, db_data in data_dicts.items():
        db_results = {}
        for sheet in sheets_to_read:
            # Get the corresponding n_steps for the current sheet
            n_steps = n_steps_map.get(sheet, 12)  # Default to 12 if the sheet is not in the map

            # Extract the best hyperparameters for this sheet and dataset
            hyperparameters_row = df_cnn_hyperparameters.loc[
                (df_cnn_hyperparameters['dataset'] == db_name) &
                (df_cnn_hyperparameters['sheet'] == sheet)
            ]

            if not hyperparameters_row.empty:
                filters = int(hyperparameters_row['filters'].values[0])
                kernel_size = int(hyperparameters_row['kernel_size'].values[0])
                dense_units = int(hyperparameters_row['dense_units'].values[0])
                epochs = int(hyperparameters_row['epochs'].values[0])
                optimizer = hyperparameters_row['optimizer'].values[0]
                loss = hyperparameters_row['loss'].values[0]
            else:
                # Default values in case hyperparameters are missing
                filters, kernel_size, dense_units, epochs, optimizer, loss = 64, 2, 50, 200, 'adam', 'mse'

            # Step 1: Extract data for the current sheet
            data = db_data[sheet]

            # Ensure only numeric data is selected and ensure it's a 2D array
            if isinstance(data, pd.DataFrame):
                data_numeric = data.select_dtypes(include=[np.number]).values
            else:
                raise ValueError(f"Expected a DataFrame for sheet {sheet}, but got {type(data)}.")

            if data_numeric.ndim == 1:
                data_numeric = data_numeric.reshape(-1, 1)

            # Step 2: Preprocess (Scaling)
            scaler = MinMaxScaler()
            if len(data_numeric) > 0:
                data_scaled = scaler.fit_transform(data_numeric)
            else:
                continue  # Skip processing if the data is empty

            # Step 3: Split into train/test
            train_size = int(len(data_scaled) * 0.7)
            train, test = data_scaled[:train_size], data_scaled[train_size:]

            if len(train) == 0 or len(test) == 0:
                continue  # Skip this sheet if there's no data to train or test

            # Step 4: Create sequences
            X_train, y_train = create_sequences(train, n_steps)
            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # Reshape for Conv1D
            X_test, y_test = create_sequences(test, n_steps)

            if X_train.size == 0 or X_test.size == 0:
                continue  # Skip if no valid sequences could be created

            # Step 5: Build and train the model with hyperparameters
            model = build_and_train_model_a(X_train, y_train, n_steps,
                                          filters=filters, kernel_size=kernel_size,
                                          dense_units=dense_units, epochs=epochs,
                                          optimizer=optimizer, loss=loss)

            # Step 6: Forecast for each test observation
            test_predictions = forecast_for_test_set(model, test, n_steps, forecast_horizon)

            # Step 7: Inverse transform the predictions and y_test
            test_predictions = scaler.inverse_transform(test_predictions)
            y_test = scaler.inverse_transform(y_test.reshape(-1, 1))

            # Step 8: Save the results in a DataFrame
            nb_obs = test_predictions.shape[0]
            columns = [f'head_{i+1}' for i in range(forecast_horizon)]
            forecast_df = pd.DataFrame(test_predictions, columns=columns)
            forecast_df.insert(0, 'nb.obs', np.arange(1, nb_obs + 1))
            forecast_df.insert(1, 'y_test', y_test[:nb_obs, 0])

            # Save results for this sheet
            save_dataframe_to_excel(forecast_df, db_name, sheet)

            # Save results for this sheet in results dictionary
            db_results[sheet] = forecast_df

        # Save results for this database
        results[db_name] = db_results

    return results

# CNN fit and forecast alpha version
def cnn_fit_and_forecast(data_var: pd.DataFrame, seq_length, n_steps, ratio_split,
                     output_file: str):

    # Step : Prepare data for CNN and create train/test splits
    def create_sequences(data, seq_length):
        X, y = [], []
        for i in range(len(data) - seq_length):
            X.append(data[i:i+seq_length])
            y.append(data[i+seq_length])
        return np.array(X), np.array(y)

    # Normalize the data
    scaler = MinMaxScaler(feature_range=(0, 1))
    gold_scaled = scaler.fit_transform(data_var[['USD']])

    # Define sequence length and split into training and testing sets
    seq_length = 4  # Use 12 months of past data to predict the next month


    X, y = create_sequences(gold_scaled, seq_length)
    train_size = int(ratio_split * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    # Reshape data for 1D CNN [samples, time steps, features]
    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

    # Step : Build and train a 1D CNN model relu  tanh
    model = Sequential([
        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(seq_length, 1)),
        Flatten(),
        Dense(50, activation='relu'),
        Dense(1)
    ])

    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
    early_stopping = EarlyStopping(patience=10, restore_best_weights=True)

    model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test),
              callbacks=[early_stopping], verbose=2)

    # Step : Function for multi-step forecasting (1-6 steps ahead)
    def multi_step_forecast(model, X_input, n_steps):
        predictions = []
        input_seq = X_input.copy()

        for step in range(n_steps):
            pred = model.predict(input_seq.reshape(1, seq_length, 1))[0]
            predictions.append(pred)
            input_seq = np.append(input_seq[1:], pred)  # Shift the window forward

        return np.array(predictions)

    # Step : Generate 1 to 6 step ahead forecasts and save in DataFrame
    n_steps = n_steps
    forecast_results = pd.DataFrame()
    y_real = scaler.inverse_transform(X_test[:, -1].reshape(-1, 1)).flatten()
    forecast_results['y_real'] = y_real  # Add the true values as 'y_real'

    for i in range(1, n_steps + 1):
        y_pred = []
        for j in range(len(X_test)):
            input_seq = X_test[j]
            forecast = multi_step_forecast(model, input_seq, i)
            y_pred.append(forecast[-1])  # Take the ith step prediction
        forecast_results[f'{i}_step'] = scaler.inverse_transform(np.array(y_pred).reshape(-1, 1)).flatten()

    # Save forecast results to Excel
    forecast_df = pd.DataFrame(forecast_results)
    forecast_df.to_excel(output_file, index=False)

    # Step : Plot test vs predicted values for each step
    plt.figure(figsize=(12, 8))

    # Get the true values for the test set
    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))

    # Plot actual values
    plt.plot(data_var['temps'][-len(y_test):], y_test_rescaled, label='Actual', color='blue')

    # Plot predicted values for each step
    for i in range(1, n_steps + 1):
        plt.plot(data_var['temps'][-len(y_test):], forecast_results[f'{i}_step'], label=f'{i}-Step Forecast')

    plt.title('Test vs Predicted Values (Multi-step Forecasting)')
    plt.xlabel('Date')
    plt.ylabel('Gold Price')
    plt.legend()
    plt.grid(True)
    plt.show()

# CNN fit and forecastion beta version
def cnn_fit_and_forecast_a(data_var: pd.DataFrame, hyperparams_file: str,
                     seq_length, n_steps, ratio_split, output_file: str):
    # Load hyperparameters
    with open(hyperparams_file, 'r') as f:
        hyperparams = json.load(f)

    # Extract hyperparameters
    filters = hyperparams.get('filters')
    kernel_size = hyperparams.get('kernel_size')
    dense_units = hyperparams.get('dense_units')
    learning_rate = hyperparams.get('learning_rate')
    epochs = hyperparams.get('tuner/epochs')

    # Prepare the data
    scaler = MinMaxScaler()
    gold_prices_scaled = scaler.fit_transform(data_var)

    def create_sequences(data, seq_length):
        X, y = [], []
        for i in range(len(data) - seq_length):
            X.append(data[i:i+seq_length])
            y.append(data[i+seq_length])
        return np.array(X), np.array(y)

    seq_length = seq_length  # Number of months to look back
    X, y = create_sequences(gold_prices_scaled, seq_length)
    X = X.reshape((X.shape[0], X.shape[1], 1))  # Reshape for CNN

    # Split data
    train_size = int(len(X) * ratio_split)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # Build the model
    model = Sequential()
    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=(seq_length, 1)))
    # model.add(MaxPooling1D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(dense_units, activation='relu'))
    model.add(Dense(1))  # Output layer

    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    early_stopping = EarlyStopping(patience=10, restore_best_weights=True)

    # Train the model
    history = model.fit(X_train, y_train, epochs=epochs, batch_size=32,
                        validation_data=(X_test, y_test),validation_split=0.1,
                        callbacks=[early_stopping], verbose=2)


    # Forecasting function
    def multi_step_forecast(model, X_input, n_steps):
        predictions = []
        input_seq = X_input.copy()

        for step in range(n_steps):
            pred = model.predict(input_seq.reshape(1, seq_length, 1))[0]
            predictions.append(pred)
            input_seq = np.append(input_seq[1:], pred)  # Shift the window forward

        return np.array(predictions)

    # Generate 1 to 6 step ahead forecasts and save in DataFrame
    n_steps = n_steps
    forecast_results = pd.DataFrame()
    y_real = scaler.inverse_transform(X_test[:, -1].reshape(-1, 1)).flatten()
    forecast_results['y_real'] = y_real  # Add the true values as 'y_real'


    for i in range(1, n_steps+1):
        y_pred = []
        for j in range(len(X_test)):
            input_seq = X_test[j]
            forecast = multi_step_forecast(model, input_seq, i)
            y_pred.append(forecast[-1])  # Take the ith step prediction
        forecast_results[f'{i}_step'] = scaler.inverse_transform(np.array(y_pred).reshape(-1, 1)).flatten()

    # Save forecast results to Excel
    forecast_df = pd.DataFrame(forecast_results)
    forecast_df.to_excel(output_file, index=False)
    print(f"Forecast results saved to {output_file}")
    return forecast_df

